{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Galaxy Nuclei detection from continuum spectra in the SDSS - A Data Science aproach to astronomy\n",
    "\n",
    "### Jaime Silva, Chihab Khnifass, Angela Montoya, Carlos Porras\n",
    "### Final project - Introduction to Data Science and Data Visualization\n",
    "### Universidad Nacional de Colombia \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Abstract:* The main objective of this Notebook is to implement different classificators and different technique of feature extraction and features selection to identify if an astronomic object is a Star, a Galaxy or a Quasar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ###  I) DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLUX_3.5600</th>\n",
       "      <th>FLUX_3.5601</th>\n",
       "      <th>FLUX_3.5602</th>\n",
       "      <th>FLUX_3.5603</th>\n",
       "      <th>FLUX_3.5604</th>\n",
       "      <th>FLUX_3.5605</th>\n",
       "      <th>FLUX_3.5606</th>\n",
       "      <th>FLUX_3.5607</th>\n",
       "      <th>FLUX_3.5608</th>\n",
       "      <th>FLUX_3.5609</th>\n",
       "      <th>...</th>\n",
       "      <th>MODEL_3.9999</th>\n",
       "      <th>MODEL_4.0000</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>class</th>\n",
       "      <th>z</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "      <th>deredSN2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specobjid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5278442614693318656</th>\n",
       "      <td>0.420023</td>\n",
       "      <td>4.738983</td>\n",
       "      <td>-1.639199</td>\n",
       "      <td>-3.824910</td>\n",
       "      <td>5.911463</td>\n",
       "      <td>5.213248</td>\n",
       "      <td>0.706732</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>5.063790</td>\n",
       "      <td>7.601273</td>\n",
       "      <td>...</td>\n",
       "      <td>3.622302</td>\n",
       "      <td>3.630245</td>\n",
       "      <td>135.986760</td>\n",
       "      <td>44.554012</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.364056</td>\n",
       "      <td>4688</td>\n",
       "      <td>56008</td>\n",
       "      <td>814</td>\n",
       "      <td>8.02797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771382263301558272</th>\n",
       "      <td>2.218949</td>\n",
       "      <td>3.160341</td>\n",
       "      <td>1.415436</td>\n",
       "      <td>-1.006396</td>\n",
       "      <td>-1.037927</td>\n",
       "      <td>1.003608</td>\n",
       "      <td>2.336863</td>\n",
       "      <td>4.245072</td>\n",
       "      <td>4.309128</td>\n",
       "      <td>2.549039</td>\n",
       "      <td>...</td>\n",
       "      <td>9.576959</td>\n",
       "      <td>9.582603</td>\n",
       "      <td>19.683591</td>\n",
       "      <td>22.994141</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.141833</td>\n",
       "      <td>5126</td>\n",
       "      <td>55923</td>\n",
       "      <td>70</td>\n",
       "      <td>7.80274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327927503851114496</th>\n",
       "      <td>8.681429</td>\n",
       "      <td>-11.233210</td>\n",
       "      <td>9.010053</td>\n",
       "      <td>4.523499</td>\n",
       "      <td>5.194839</td>\n",
       "      <td>2.488315</td>\n",
       "      <td>1.749791</td>\n",
       "      <td>6.603962</td>\n",
       "      <td>0.682046</td>\n",
       "      <td>-5.839457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927221</td>\n",
       "      <td>0.942549</td>\n",
       "      <td>163.563320</td>\n",
       "      <td>2.509434</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.569191</td>\n",
       "      <td>4732</td>\n",
       "      <td>55648</td>\n",
       "      <td>615</td>\n",
       "      <td>8.41838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965265697100025856</th>\n",
       "      <td>-0.925472</td>\n",
       "      <td>-1.188074</td>\n",
       "      <td>-0.806738</td>\n",
       "      <td>0.526379</td>\n",
       "      <td>-0.597940</td>\n",
       "      <td>-2.700016</td>\n",
       "      <td>-0.531075</td>\n",
       "      <td>0.792433</td>\n",
       "      <td>1.923451</td>\n",
       "      <td>0.609599</td>\n",
       "      <td>...</td>\n",
       "      <td>1.849819</td>\n",
       "      <td>1.853152</td>\n",
       "      <td>338.419100</td>\n",
       "      <td>5.288300</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.479037</td>\n",
       "      <td>4410</td>\n",
       "      <td>56187</td>\n",
       "      <td>171</td>\n",
       "      <td>8.98737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10566641405220114432</th>\n",
       "      <td>1.233917</td>\n",
       "      <td>1.741172</td>\n",
       "      <td>-4.932346</td>\n",
       "      <td>23.809027</td>\n",
       "      <td>-6.837817</td>\n",
       "      <td>6.812706</td>\n",
       "      <td>13.774871</td>\n",
       "      <td>-13.514325</td>\n",
       "      <td>1.792809</td>\n",
       "      <td>-8.490336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316690</td>\n",
       "      <td>0.309001</td>\n",
       "      <td>34.242564</td>\n",
       "      <td>0.342847</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.847324</td>\n",
       "      <td>9385</td>\n",
       "      <td>58099</td>\n",
       "      <td>257</td>\n",
       "      <td>7.93252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581828771516534784</th>\n",
       "      <td>14.946466</td>\n",
       "      <td>14.448165</td>\n",
       "      <td>9.885964</td>\n",
       "      <td>12.755141</td>\n",
       "      <td>24.863268</td>\n",
       "      <td>9.458821</td>\n",
       "      <td>24.251238</td>\n",
       "      <td>13.435109</td>\n",
       "      <td>10.934933</td>\n",
       "      <td>25.792227</td>\n",
       "      <td>...</td>\n",
       "      <td>6.105735</td>\n",
       "      <td>6.120405</td>\n",
       "      <td>230.352940</td>\n",
       "      <td>46.736050</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>6734</td>\n",
       "      <td>56386</td>\n",
       "      <td>68</td>\n",
       "      <td>9.37717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6537033239110111232</th>\n",
       "      <td>2.423653</td>\n",
       "      <td>1.083916</td>\n",
       "      <td>-3.080085</td>\n",
       "      <td>2.254264</td>\n",
       "      <td>-0.824761</td>\n",
       "      <td>1.631145</td>\n",
       "      <td>1.776871</td>\n",
       "      <td>-0.308378</td>\n",
       "      <td>1.254579</td>\n",
       "      <td>1.345754</td>\n",
       "      <td>...</td>\n",
       "      <td>15.598204</td>\n",
       "      <td>14.924174</td>\n",
       "      <td>145.694050</td>\n",
       "      <td>30.948545</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>5806</td>\n",
       "      <td>56310</td>\n",
       "      <td>212</td>\n",
       "      <td>9.67117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7267817046448295936</th>\n",
       "      <td>-0.969628</td>\n",
       "      <td>1.281691</td>\n",
       "      <td>-3.111764</td>\n",
       "      <td>-3.392078</td>\n",
       "      <td>-0.017491</td>\n",
       "      <td>1.602356</td>\n",
       "      <td>0.154054</td>\n",
       "      <td>0.068322</td>\n",
       "      <td>1.264121</td>\n",
       "      <td>-4.809132</td>\n",
       "      <td>...</td>\n",
       "      <td>3.841213</td>\n",
       "      <td>3.938601</td>\n",
       "      <td>154.112090</td>\n",
       "      <td>30.801705</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>6455</td>\n",
       "      <td>56370</td>\n",
       "      <td>484</td>\n",
       "      <td>9.31134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321222078156263424</th>\n",
       "      <td>9.640415</td>\n",
       "      <td>1.727048</td>\n",
       "      <td>0.548067</td>\n",
       "      <td>6.006572</td>\n",
       "      <td>1.666739</td>\n",
       "      <td>3.203077</td>\n",
       "      <td>4.990945</td>\n",
       "      <td>5.554278</td>\n",
       "      <td>2.745045</td>\n",
       "      <td>1.390249</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053401</td>\n",
       "      <td>2.056720</td>\n",
       "      <td>170.341720</td>\n",
       "      <td>-0.390613</td>\n",
       "      <td>STAR</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>3838</td>\n",
       "      <td>55588</td>\n",
       "      <td>66</td>\n",
       "      <td>7.97551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157720964400697344</th>\n",
       "      <td>-1.139163</td>\n",
       "      <td>-2.785214</td>\n",
       "      <td>5.132689</td>\n",
       "      <td>-0.955416</td>\n",
       "      <td>-1.105428</td>\n",
       "      <td>1.473216</td>\n",
       "      <td>2.029250</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>-0.772318</td>\n",
       "      <td>7.545609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925184</td>\n",
       "      <td>0.927923</td>\n",
       "      <td>219.289410</td>\n",
       "      <td>17.830454</td>\n",
       "      <td>STAR</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>5469</td>\n",
       "      <td>56037</td>\n",
       "      <td>634</td>\n",
       "      <td>9.26914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22537 rows × 30815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      FLUX_3.5600  FLUX_3.5601  FLUX_3.5602  FLUX_3.5603  \\\n",
       "specobjid                                                                  \n",
       "5278442614693318656      0.420023     4.738983    -1.639199    -3.824910   \n",
       "5771382263301558272      2.218949     3.160341     1.415436    -1.006396   \n",
       "5327927503851114496      8.681429   -11.233210     9.010053     4.523499   \n",
       "4965265697100025856     -0.925472    -1.188074    -0.806738     0.526379   \n",
       "10566641405220114432     1.233917     1.741172    -4.932346    23.809027   \n",
       "...                           ...          ...          ...          ...   \n",
       "7581828771516534784     14.946466    14.448165     9.885964    12.755141   \n",
       "6537033239110111232      2.423653     1.083916    -3.080085     2.254264   \n",
       "7267817046448295936     -0.969628     1.281691    -3.111764    -3.392078   \n",
       "4321222078156263424      9.640415     1.727048     0.548067     6.006572   \n",
       "6157720964400697344     -1.139163    -2.785214     5.132689    -0.955416   \n",
       "\n",
       "                      FLUX_3.5604  FLUX_3.5605  FLUX_3.5606  FLUX_3.5607  \\\n",
       "specobjid                                                                  \n",
       "5278442614693318656      5.911463     5.213248     0.706732     0.105100   \n",
       "5771382263301558272     -1.037927     1.003608     2.336863     4.245072   \n",
       "5327927503851114496      5.194839     2.488315     1.749791     6.603962   \n",
       "4965265697100025856     -0.597940    -2.700016    -0.531075     0.792433   \n",
       "10566641405220114432    -6.837817     6.812706    13.774871   -13.514325   \n",
       "...                           ...          ...          ...          ...   \n",
       "7581828771516534784     24.863268     9.458821    24.251238    13.435109   \n",
       "6537033239110111232     -0.824761     1.631145     1.776871    -0.308378   \n",
       "7267817046448295936     -0.017491     1.602356     0.154054     0.068322   \n",
       "4321222078156263424      1.666739     3.203077     4.990945     5.554278   \n",
       "6157720964400697344     -1.105428     1.473216     2.029250     0.200195   \n",
       "\n",
       "                      FLUX_3.5608  FLUX_3.5609  ...  MODEL_3.9999  \\\n",
       "specobjid                                       ...                 \n",
       "5278442614693318656      5.063790     7.601273  ...      3.622302   \n",
       "5771382263301558272      4.309128     2.549039  ...      9.576959   \n",
       "5327927503851114496      0.682046    -5.839457  ...      0.927221   \n",
       "4965265697100025856      1.923451     0.609599  ...      1.849819   \n",
       "10566641405220114432     1.792809    -8.490336  ...      0.316690   \n",
       "...                           ...          ...  ...           ...   \n",
       "7581828771516534784     10.934933    25.792227  ...      6.105735   \n",
       "6537033239110111232      1.254579     1.345754  ...     15.598204   \n",
       "7267817046448295936      1.264121    -4.809132  ...      3.841213   \n",
       "4321222078156263424      2.745045     1.390249  ...      2.053401   \n",
       "6157720964400697344     -0.772318     7.545609  ...      0.925184   \n",
       "\n",
       "                      MODEL_4.0000          ra        dec   class         z  \\\n",
       "specobjid                                                                     \n",
       "5278442614693318656       3.630245  135.986760  44.554012  GALAXY  0.364056   \n",
       "5771382263301558272       9.582603   19.683591  22.994141  GALAXY  0.141833   \n",
       "5327927503851114496       0.942549  163.563320   2.509434  GALAXY  0.569191   \n",
       "4965265697100025856       1.853152  338.419100   5.288300  GALAXY  0.479037   \n",
       "10566641405220114432      0.309001   34.242564   0.342847  GALAXY  0.847324   \n",
       "...                            ...         ...        ...     ...       ...   \n",
       "7581828771516534784       6.120405  230.352940  46.736050    STAR -0.000483   \n",
       "6537033239110111232      14.924174  145.694050  30.948545    STAR  0.000180   \n",
       "7267817046448295936       3.938601  154.112090  30.801705    STAR  0.000414   \n",
       "4321222078156263424       2.056720  170.341720  -0.390613    STAR  0.000104   \n",
       "6157720964400697344       0.927923  219.289410  17.830454    STAR -0.000641   \n",
       "\n",
       "                      plate    mjd  fiberid  deredSN2  \n",
       "specobjid                                              \n",
       "5278442614693318656    4688  56008      814   8.02797  \n",
       "5771382263301558272    5126  55923       70   7.80274  \n",
       "5327927503851114496    4732  55648      615   8.41838  \n",
       "4965265697100025856    4410  56187      171   8.98737  \n",
       "10566641405220114432   9385  58099      257   7.93252  \n",
       "...                     ...    ...      ...       ...  \n",
       "7581828771516534784    6734  56386       68   9.37717  \n",
       "6537033239110111232    5806  56310      212   9.67117  \n",
       "7267817046448295936    6455  56370      484   9.31134  \n",
       "4321222078156263424    3838  55588       66   7.97551  \n",
       "6157720964400697344    5469  56037      634   9.26914  \n",
       "\n",
       "[22537 rows x 30815 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset mas grande que antes\n",
    "df=joblib.load(\"data/medium.joblib\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete useless features id\n",
    "df.drop([ 'fiberid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete raw with NaN\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode class label (Quasar,Galaxy and star) to integer (0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "df_fe = df\n",
    "\n",
    "# encode class labels to integers\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df_fe['class'])\n",
    "df_fe['class'] = y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "specobjid\n",
       "5278442614693318656     0\n",
       "5771382263301558272     0\n",
       "5327927503851114496     0\n",
       "4965265697100025856     0\n",
       "10566641405220114432    0\n",
       "                       ..\n",
       "7581828771516534784     2\n",
       "6537033239110111232     2\n",
       "7267817046448295936     2\n",
       "4321222078156263424     2\n",
       "6157720964400697344     2\n",
       "Name: class, Length: 22345, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result of the encoding\n",
    "df_fe['class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the important number of features and Data we wille normalize the dataset for a fastest computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data between 0 and 1 for a  fastest computation\n",
    "scaler = MinMaxScaler()\n",
    "sdss = scaler.fit_transform(df_fe.drop('class', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLUX_3.5600    0\n",
       "FLUX_3.5601    0\n",
       "FLUX_3.5602    0\n",
       "FLUX_3.5603    0\n",
       "FLUX_3.5604    0\n",
       "              ..\n",
       "class          0\n",
       "z              0\n",
       "plate          0\n",
       "mjd            0\n",
       "deredSN2       0\n",
       "Length: 30814, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verification that there is no NaN\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  II)Reduction of the number of features\n",
    "   ###  a)Feature extraction thanks to a dimensional reduction\n",
    "   ###  1)LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. This technique for dimensional reduction could be good for our project because we have a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the set of all features\n",
    "X=sdss\n",
    "#y is the target class\n",
    "y=df_fe['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "#feature extraction to reduce the number of features\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#Split the dataset between train and test to aply LDA\n",
    "X_trainl, X_testl, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Create an LDA that will reduce the data down to 2 feature becaus we have to choose: number of class - 1 = 2 \n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "\n",
    "# run an LDA and use it to transform the features on the train set\n",
    "X_train = lda.fit(X_trainl, y_train).transform(X_trainl)\n",
    "\n",
    "X_test =  lda.transform(X_testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.945536</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.464554</td>\n",
       "      <td>-5.649419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.685897</td>\n",
       "      <td>10.894139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.013062</td>\n",
       "      <td>2.593742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.317703</td>\n",
       "      <td>-2.200197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14966</th>\n",
       "      <td>6.615082</td>\n",
       "      <td>-0.035711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14967</th>\n",
       "      <td>-4.155076</td>\n",
       "      <td>9.744721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14968</th>\n",
       "      <td>9.324032</td>\n",
       "      <td>-0.040497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14969</th>\n",
       "      <td>6.871570</td>\n",
       "      <td>0.545420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14970</th>\n",
       "      <td>7.159752</td>\n",
       "      <td>0.753004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14971 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1\n",
       "0      5.945536   0.683900\n",
       "1     -3.464554  -5.649419\n",
       "2     -3.685897  10.894139\n",
       "3      6.013062   2.593742\n",
       "4      7.317703  -2.200197\n",
       "...         ...        ...\n",
       "14966  6.615082  -0.035711\n",
       "14967 -4.155076   9.744721\n",
       "14968  9.324032  -0.040497\n",
       "14969  6.871570   0.545420\n",
       "14970  7.159752   0.753004\n",
       "\n",
       "[14971 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result of the LDA\n",
    "X_trainp = pd.DataFrame(X_train)\n",
    "X_trainp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-12.251449</td>\n",
       "      <td>-5.219652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.990336</td>\n",
       "      <td>1.559395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.808860</td>\n",
       "      <td>9.928623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.827382</td>\n",
       "      <td>9.912041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.525756</td>\n",
       "      <td>2.066852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369</th>\n",
       "      <td>7.950091</td>\n",
       "      <td>1.006240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>5.757429</td>\n",
       "      <td>7.325681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>-4.666276</td>\n",
       "      <td>-8.227756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>-9.442462</td>\n",
       "      <td>8.850452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>1.104071</td>\n",
       "      <td>1.237817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7374 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0    -12.251449 -5.219652\n",
       "1      6.990336  1.559395\n",
       "2     -1.808860  9.928623\n",
       "3     -0.827382  9.912041\n",
       "4     -2.525756  2.066852\n",
       "...         ...       ...\n",
       "7369   7.950091  1.006240\n",
       "7370   5.757429  7.325681\n",
       "7371  -4.666276 -8.227756\n",
       "7372  -9.442462  8.850452\n",
       "7373   1.104071  1.237817\n",
       "\n",
       "[7374 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testp = pd.DataFrame(X_test)\n",
    "X_testp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  2)PCA\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed.Contrary to LDA it's unsupervised but we can have more features on the final dataset.\n",
    "we reduce the space to 30 features because we know that the key emission and absorption lines are about 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Principal Component Analysis \n",
    "pca = PCA(n_components=30)\n",
    "ugriz = pca.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.110939</td>\n",
       "      <td>2.303279</td>\n",
       "      <td>1.331596</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>-1.197247</td>\n",
       "      <td>-3.654767</td>\n",
       "      <td>-1.692724</td>\n",
       "      <td>1.268413</td>\n",
       "      <td>-0.987753</td>\n",
       "      <td>-1.124890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939284</td>\n",
       "      <td>-0.081272</td>\n",
       "      <td>0.681660</td>\n",
       "      <td>-0.579545</td>\n",
       "      <td>0.487948</td>\n",
       "      <td>-1.110181</td>\n",
       "      <td>1.349614</td>\n",
       "      <td>-1.070154</td>\n",
       "      <td>0.054412</td>\n",
       "      <td>-1.383752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.403183</td>\n",
       "      <td>-1.173265</td>\n",
       "      <td>3.922785</td>\n",
       "      <td>6.692560</td>\n",
       "      <td>-0.091238</td>\n",
       "      <td>-2.159926</td>\n",
       "      <td>1.657917</td>\n",
       "      <td>-1.133048</td>\n",
       "      <td>-1.139135</td>\n",
       "      <td>-1.275796</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.330168</td>\n",
       "      <td>-0.546807</td>\n",
       "      <td>0.069524</td>\n",
       "      <td>-0.457547</td>\n",
       "      <td>-0.423085</td>\n",
       "      <td>0.855163</td>\n",
       "      <td>1.303976</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.701021</td>\n",
       "      <td>0.134902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.201880</td>\n",
       "      <td>3.376301</td>\n",
       "      <td>2.049639</td>\n",
       "      <td>-0.478147</td>\n",
       "      <td>-3.743812</td>\n",
       "      <td>-0.646420</td>\n",
       "      <td>-0.682998</td>\n",
       "      <td>-2.481045</td>\n",
       "      <td>-2.595568</td>\n",
       "      <td>1.268277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813707</td>\n",
       "      <td>-0.275541</td>\n",
       "      <td>-0.475717</td>\n",
       "      <td>-0.783347</td>\n",
       "      <td>-0.789979</td>\n",
       "      <td>0.124059</td>\n",
       "      <td>-1.112134</td>\n",
       "      <td>0.683518</td>\n",
       "      <td>0.155799</td>\n",
       "      <td>0.235168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.995609</td>\n",
       "      <td>-2.291103</td>\n",
       "      <td>-7.266586</td>\n",
       "      <td>-0.686463</td>\n",
       "      <td>-0.764281</td>\n",
       "      <td>0.348354</td>\n",
       "      <td>2.787221</td>\n",
       "      <td>-3.270681</td>\n",
       "      <td>0.734808</td>\n",
       "      <td>1.944188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.507693</td>\n",
       "      <td>-0.844031</td>\n",
       "      <td>-0.631053</td>\n",
       "      <td>0.338471</td>\n",
       "      <td>-0.884727</td>\n",
       "      <td>-0.042189</td>\n",
       "      <td>1.239950</td>\n",
       "      <td>-0.192652</td>\n",
       "      <td>-0.795036</td>\n",
       "      <td>0.047116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.135595</td>\n",
       "      <td>-4.523934</td>\n",
       "      <td>-2.337166</td>\n",
       "      <td>0.138769</td>\n",
       "      <td>1.171451</td>\n",
       "      <td>-2.199384</td>\n",
       "      <td>-0.816777</td>\n",
       "      <td>2.637739</td>\n",
       "      <td>0.608089</td>\n",
       "      <td>2.779027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452637</td>\n",
       "      <td>-0.749700</td>\n",
       "      <td>0.459814</td>\n",
       "      <td>0.354968</td>\n",
       "      <td>-0.395864</td>\n",
       "      <td>-0.225875</td>\n",
       "      <td>-1.148926</td>\n",
       "      <td>0.214742</td>\n",
       "      <td>-0.209782</td>\n",
       "      <td>-0.678130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22340</th>\n",
       "      <td>3.899000</td>\n",
       "      <td>-2.486519</td>\n",
       "      <td>2.054269</td>\n",
       "      <td>4.340261</td>\n",
       "      <td>-2.044900</td>\n",
       "      <td>1.407312</td>\n",
       "      <td>-2.320249</td>\n",
       "      <td>-2.425881</td>\n",
       "      <td>-0.370204</td>\n",
       "      <td>-0.678138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563309</td>\n",
       "      <td>0.528110</td>\n",
       "      <td>1.217560</td>\n",
       "      <td>0.340938</td>\n",
       "      <td>0.112629</td>\n",
       "      <td>0.410356</td>\n",
       "      <td>1.497754</td>\n",
       "      <td>0.326552</td>\n",
       "      <td>-0.307013</td>\n",
       "      <td>1.329098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22341</th>\n",
       "      <td>4.282459</td>\n",
       "      <td>-2.497640</td>\n",
       "      <td>1.185793</td>\n",
       "      <td>5.877086</td>\n",
       "      <td>-2.901012</td>\n",
       "      <td>1.277598</td>\n",
       "      <td>1.392817</td>\n",
       "      <td>-1.258173</td>\n",
       "      <td>-1.795758</td>\n",
       "      <td>0.347940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277781</td>\n",
       "      <td>0.925139</td>\n",
       "      <td>0.977039</td>\n",
       "      <td>1.724358</td>\n",
       "      <td>0.363524</td>\n",
       "      <td>0.140128</td>\n",
       "      <td>-0.468509</td>\n",
       "      <td>0.688251</td>\n",
       "      <td>-0.331348</td>\n",
       "      <td>0.975615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22342</th>\n",
       "      <td>4.543934</td>\n",
       "      <td>-2.115460</td>\n",
       "      <td>2.234635</td>\n",
       "      <td>2.449535</td>\n",
       "      <td>8.786118</td>\n",
       "      <td>0.813442</td>\n",
       "      <td>-2.457396</td>\n",
       "      <td>-2.285924</td>\n",
       "      <td>-2.139477</td>\n",
       "      <td>-1.854662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339078</td>\n",
       "      <td>0.521111</td>\n",
       "      <td>0.516404</td>\n",
       "      <td>0.294928</td>\n",
       "      <td>0.866013</td>\n",
       "      <td>-1.691511</td>\n",
       "      <td>-2.071605</td>\n",
       "      <td>0.834924</td>\n",
       "      <td>-0.326775</td>\n",
       "      <td>0.304468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22343</th>\n",
       "      <td>20.916397</td>\n",
       "      <td>-0.644546</td>\n",
       "      <td>-0.271125</td>\n",
       "      <td>0.807168</td>\n",
       "      <td>-1.186007</td>\n",
       "      <td>6.049641</td>\n",
       "      <td>-2.559389</td>\n",
       "      <td>-0.042294</td>\n",
       "      <td>-0.427126</td>\n",
       "      <td>0.224475</td>\n",
       "      <td>...</td>\n",
       "      <td>1.498661</td>\n",
       "      <td>0.991513</td>\n",
       "      <td>-1.140122</td>\n",
       "      <td>1.050129</td>\n",
       "      <td>1.218625</td>\n",
       "      <td>0.868098</td>\n",
       "      <td>1.326045</td>\n",
       "      <td>-0.984999</td>\n",
       "      <td>-0.730614</td>\n",
       "      <td>-1.028529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22344</th>\n",
       "      <td>-5.261250</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.412301</td>\n",
       "      <td>-5.279493</td>\n",
       "      <td>-2.179923</td>\n",
       "      <td>-0.487475</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.381430</td>\n",
       "      <td>1.117196</td>\n",
       "      <td>-0.132739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.686896</td>\n",
       "      <td>0.699884</td>\n",
       "      <td>0.323937</td>\n",
       "      <td>0.461939</td>\n",
       "      <td>-0.487861</td>\n",
       "      <td>-0.741587</td>\n",
       "      <td>1.020900</td>\n",
       "      <td>0.095530</td>\n",
       "      <td>0.897110</td>\n",
       "      <td>0.835667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22345 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0      -7.110939  2.303279  1.331596  0.241424 -1.197247 -3.654767 -1.692724   \n",
       "1      -3.403183 -1.173265  3.922785  6.692560 -0.091238 -2.159926  1.657917   \n",
       "2      -3.201880  3.376301  2.049639 -0.478147 -3.743812 -0.646420 -0.682998   \n",
       "3      -4.995609 -2.291103 -7.266586 -0.686463 -0.764281  0.348354  2.787221   \n",
       "4      -0.135595 -4.523934 -2.337166  0.138769  1.171451 -2.199384 -0.816777   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "22340   3.899000 -2.486519  2.054269  4.340261 -2.044900  1.407312 -2.320249   \n",
       "22341   4.282459 -2.497640  1.185793  5.877086 -2.901012  1.277598  1.392817   \n",
       "22342   4.543934 -2.115460  2.234635  2.449535  8.786118  0.813442 -2.457396   \n",
       "22343  20.916397 -0.644546 -0.271125  0.807168 -1.186007  6.049641 -2.559389   \n",
       "22344  -5.261250  0.003496  0.412301 -5.279493 -2.179923 -0.487475 -0.230186   \n",
       "\n",
       "             7         8         9   ...        20        21        22  \\\n",
       "0      1.268413 -0.987753 -1.124890  ...  0.939284 -0.081272  0.681660   \n",
       "1     -1.133048 -1.139135 -1.275796  ... -1.330168 -0.546807  0.069524   \n",
       "2     -2.481045 -2.595568  1.268277  ... -0.813707 -0.275541 -0.475717   \n",
       "3     -3.270681  0.734808  1.944188  ... -0.507693 -0.844031 -0.631053   \n",
       "4      2.637739  0.608089  2.779027  ...  0.452637 -0.749700  0.459814   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "22340 -2.425881 -0.370204 -0.678138  ...  0.563309  0.528110  1.217560   \n",
       "22341 -1.258173 -1.795758  0.347940  ...  0.277781  0.925139  0.977039   \n",
       "22342 -2.285924 -2.139477 -1.854662  ...  0.339078  0.521111  0.516404   \n",
       "22343 -0.042294 -0.427126  0.224475  ...  1.498661  0.991513 -1.140122   \n",
       "22344  0.381430  1.117196 -0.132739  ... -0.686896  0.699884  0.323937   \n",
       "\n",
       "             23        24        25        26        27        28        29  \n",
       "0     -0.579545  0.487948 -1.110181  1.349614 -1.070154  0.054412 -1.383752  \n",
       "1     -0.457547 -0.423085  0.855163  1.303976  0.007251  0.701021  0.134902  \n",
       "2     -0.783347 -0.789979  0.124059 -1.112134  0.683518  0.155799  0.235168  \n",
       "3      0.338471 -0.884727 -0.042189  1.239950 -0.192652 -0.795036  0.047116  \n",
       "4      0.354968 -0.395864 -0.225875 -1.148926  0.214742 -0.209782 -0.678130  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "22340  0.340938  0.112629  0.410356  1.497754  0.326552 -0.307013  1.329098  \n",
       "22341  1.724358  0.363524  0.140128 -0.468509  0.688251 -0.331348  0.975615  \n",
       "22342  0.294928  0.866013 -1.691511 -2.071605  0.834924 -0.326775  0.304468  \n",
       "22343  1.050129  1.218625  0.868098  1.326045 -0.984999 -0.730614 -1.028529  \n",
       "22344  0.461939 -0.487861 -0.741587  1.020900  0.095530  0.897110  0.835667  \n",
       "\n",
       "[22345 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result of the PCA\n",
    "Xpca=pd.DataFrame(ugriz)\n",
    "Xpca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b)Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature selection we will select the 30 best features for the same reasons as the PCA methods.To choose the best feature we will use the python function which is SLECTKBEST.\n",
    "We will use chi2 as a score function because we have a classification problem, SelectKBest will compute the chi2 statistic between each feature of X and y (assumed to be class labels). A small value will mean the feature is independent of y. A large value will mean the feature is non-randomly related to y, and so likely to provide important information. Only 30 features will be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# find best scored 30 features\n",
    "select_feature = SelectKBest(chi2, k=30).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=30, score_func=<function chi2 at 0x7fe7c8c75b70>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We select the 30 best columns on the original dataset\n",
    "cols = select_feature.get_support(indices=True)\n",
    "Xbest = df_fe.drop('class', axis=1).iloc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AND_MASK_3.6131</th>\n",
       "      <th>AND_MASK_3.6376</th>\n",
       "      <th>AND_MASK_3.6377</th>\n",
       "      <th>AND_MASK_3.6378</th>\n",
       "      <th>AND_MASK_3.6393</th>\n",
       "      <th>AND_MASK_3.6394</th>\n",
       "      <th>AND_MASK_3.6395</th>\n",
       "      <th>AND_MASK_3.6868</th>\n",
       "      <th>AND_MASK_3.6869</th>\n",
       "      <th>AND_MASK_3.6870</th>\n",
       "      <th>...</th>\n",
       "      <th>OR_MASK_3.6868</th>\n",
       "      <th>OR_MASK_3.6869</th>\n",
       "      <th>OR_MASK_3.6870</th>\n",
       "      <th>OR_MASK_3.6871</th>\n",
       "      <th>OR_MASK_3.6872</th>\n",
       "      <th>OR_MASK_3.8272</th>\n",
       "      <th>OR_MASK_3.8273</th>\n",
       "      <th>OR_MASK_3.8274</th>\n",
       "      <th>OR_MASK_3.8284</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specobjid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5278442614693318656</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.364056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771382263301558272</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84475904.0</td>\n",
       "      <td>84475904.0</td>\n",
       "      <td>84475904.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.141833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327927503851114496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.569191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4965265697100025856</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84410368.0</td>\n",
       "      <td>84410368.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.479037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10566641405220114432</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84475904.0</td>\n",
       "      <td>84475904.0</td>\n",
       "      <td>84475904.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.847324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581828771516534784</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83886084.0</td>\n",
       "      <td>83886084.0</td>\n",
       "      <td>83886084.0</td>\n",
       "      <td>117440516.0</td>\n",
       "      <td>-0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6537033239110111232</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7267817046448295936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321222078156263424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117440512.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6157720964400697344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>8388608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>524288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>83886080.0</td>\n",
       "      <td>-0.000641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22345 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      AND_MASK_3.6131  AND_MASK_3.6376  AND_MASK_3.6377  \\\n",
       "specobjid                                                                 \n",
       "5278442614693318656               0.0              0.0              0.0   \n",
       "5771382263301558272               0.0              0.0              0.0   \n",
       "5327927503851114496               0.0              0.0              0.0   \n",
       "4965265697100025856               0.0              0.0              0.0   \n",
       "10566641405220114432              0.0              0.0              0.0   \n",
       "...                               ...              ...              ...   \n",
       "7581828771516534784               4.0              4.0              4.0   \n",
       "6537033239110111232               0.0              0.0              0.0   \n",
       "7267817046448295936               0.0              0.0              0.0   \n",
       "4321222078156263424               0.0              0.0              0.0   \n",
       "6157720964400697344               0.0              0.0              0.0   \n",
       "\n",
       "                      AND_MASK_3.6378  AND_MASK_3.6393  AND_MASK_3.6394  \\\n",
       "specobjid                                                                 \n",
       "5278442614693318656               0.0        8388608.0        8388608.0   \n",
       "5771382263301558272               0.0        8388608.0        8388608.0   \n",
       "5327927503851114496               0.0              0.0              0.0   \n",
       "4965265697100025856               0.0        8388608.0        8388608.0   \n",
       "10566641405220114432              0.0        8388608.0        8388608.0   \n",
       "...                               ...              ...              ...   \n",
       "7581828771516534784               4.0              4.0              4.0   \n",
       "6537033239110111232               0.0              0.0        8388608.0   \n",
       "7267817046448295936               0.0        8388608.0        8388608.0   \n",
       "4321222078156263424               0.0              0.0              0.0   \n",
       "6157720964400697344               0.0              0.0        8388608.0   \n",
       "\n",
       "                      AND_MASK_3.6395  AND_MASK_3.6868  AND_MASK_3.6869  \\\n",
       "specobjid                                                                 \n",
       "5278442614693318656         8388608.0              0.0              0.0   \n",
       "5771382263301558272         8388608.0              0.0              0.0   \n",
       "5327927503851114496               0.0              0.0              0.0   \n",
       "4965265697100025856         8388608.0              0.0              0.0   \n",
       "10566641405220114432        8388608.0              0.0              0.0   \n",
       "...                               ...              ...              ...   \n",
       "7581828771516534784               4.0              4.0              4.0   \n",
       "6537033239110111232         8388608.0              0.0              0.0   \n",
       "7267817046448295936               0.0              0.0              0.0   \n",
       "4321222078156263424               0.0              0.0              0.0   \n",
       "6157720964400697344         8388608.0              0.0              0.0   \n",
       "\n",
       "                      AND_MASK_3.6870  ...  OR_MASK_3.6868  OR_MASK_3.6869  \\\n",
       "specobjid                              ...                                   \n",
       "5278442614693318656               0.0  ...             0.0             0.0   \n",
       "5771382263301558272               0.0  ...             0.0             0.0   \n",
       "5327927503851114496               0.0  ...             0.0             0.0   \n",
       "4965265697100025856               0.0  ...             0.0             0.0   \n",
       "10566641405220114432              0.0  ...             0.0             0.0   \n",
       "...                               ...  ...             ...             ...   \n",
       "7581828771516534784               4.0  ...             4.0             4.0   \n",
       "6537033239110111232               0.0  ...             0.0             0.0   \n",
       "7267817046448295936               0.0  ...             0.0             0.0   \n",
       "4321222078156263424               0.0  ...             0.0             0.0   \n",
       "6157720964400697344               0.0  ...        524288.0             0.0   \n",
       "\n",
       "                      OR_MASK_3.6870  OR_MASK_3.6871  OR_MASK_3.6872  \\\n",
       "specobjid                                                              \n",
       "5278442614693318656              0.0             0.0             0.0   \n",
       "5771382263301558272              0.0             0.0             0.0   \n",
       "5327927503851114496              0.0             0.0             0.0   \n",
       "4965265697100025856              0.0             0.0             0.0   \n",
       "10566641405220114432             0.0             0.0             0.0   \n",
       "...                              ...             ...             ...   \n",
       "7581828771516534784              4.0             4.0             4.0   \n",
       "6537033239110111232              0.0             0.0             0.0   \n",
       "7267817046448295936              0.0             0.0             0.0   \n",
       "4321222078156263424              0.0             0.0             0.0   \n",
       "6157720964400697344              0.0             0.0             0.0   \n",
       "\n",
       "                      OR_MASK_3.8272  OR_MASK_3.8273  OR_MASK_3.8274  \\\n",
       "specobjid                                                              \n",
       "5278442614693318656       83886080.0      83886080.0      83886080.0   \n",
       "5771382263301558272       84475904.0      84475904.0      84475904.0   \n",
       "5327927503851114496       83886080.0      83886080.0      83886080.0   \n",
       "4965265697100025856       84410368.0      84410368.0      83886080.0   \n",
       "10566641405220114432      84475904.0      84475904.0      84475904.0   \n",
       "...                              ...             ...             ...   \n",
       "7581828771516534784       83886084.0      83886084.0      83886084.0   \n",
       "6537033239110111232       83886080.0      83886080.0      83886080.0   \n",
       "7267817046448295936       83886080.0      83886080.0      83886080.0   \n",
       "4321222078156263424      117440512.0      83886080.0      83886080.0   \n",
       "6157720964400697344       83886080.0      83886080.0      83886080.0   \n",
       "\n",
       "                      OR_MASK_3.8284         z  \n",
       "specobjid                                       \n",
       "5278442614693318656       83886080.0  0.364056  \n",
       "5771382263301558272       83886080.0  0.141833  \n",
       "5327927503851114496       83886080.0  0.569191  \n",
       "4965265697100025856       83886080.0  0.479037  \n",
       "10566641405220114432      83886080.0  0.847324  \n",
       "...                              ...       ...  \n",
       "7581828771516534784      117440516.0 -0.000483  \n",
       "6537033239110111232       83886080.0  0.000180  \n",
       "7267817046448295936       83886080.0  0.000414  \n",
       "4321222078156263424       83886080.0  0.000104  \n",
       "6157720964400697344       83886080.0 -0.000641  \n",
       "\n",
       "[22345 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we can see the 30 best features\n",
    "Xbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCORE OF THE FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6131</th>\n",
       "      <td>9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6376</th>\n",
       "      <td>9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6377</th>\n",
       "      <td>9579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6378</th>\n",
       "      <td>9580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6393</th>\n",
       "      <td>9595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6394</th>\n",
       "      <td>9596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6395</th>\n",
       "      <td>9597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6868</th>\n",
       "      <td>10070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6869</th>\n",
       "      <td>10071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.6870</th>\n",
       "      <td>10072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AND_MASK_3.9793</th>\n",
       "      <td>12995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.5713</th>\n",
       "      <td>13316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.5714</th>\n",
       "      <td>13317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.5715</th>\n",
       "      <td>13318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.5716</th>\n",
       "      <td>13319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.5717</th>\n",
       "      <td>13320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6376</th>\n",
       "      <td>13979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6377</th>\n",
       "      <td>13980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6378</th>\n",
       "      <td>13981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6867</th>\n",
       "      <td>14470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6868</th>\n",
       "      <td>14471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6869</th>\n",
       "      <td>14472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6870</th>\n",
       "      <td>14473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6871</th>\n",
       "      <td>14474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.6872</th>\n",
       "      <td>14475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.8272</th>\n",
       "      <td>15875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.8273</th>\n",
       "      <td>15876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.8274</th>\n",
       "      <td>15877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OR_MASK_3.8284</th>\n",
       "      <td>15887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>30809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 SCORE OF THE FEATURE\n",
       "AND_MASK_3.6131                  9333\n",
       "AND_MASK_3.6376                  9578\n",
       "AND_MASK_3.6377                  9579\n",
       "AND_MASK_3.6378                  9580\n",
       "AND_MASK_3.6393                  9595\n",
       "AND_MASK_3.6394                  9596\n",
       "AND_MASK_3.6395                  9597\n",
       "AND_MASK_3.6868                 10070\n",
       "AND_MASK_3.6869                 10071\n",
       "AND_MASK_3.6870                 10072\n",
       "AND_MASK_3.9793                 12995\n",
       "OR_MASK_3.5713                  13316\n",
       "OR_MASK_3.5714                  13317\n",
       "OR_MASK_3.5715                  13318\n",
       "OR_MASK_3.5716                  13319\n",
       "OR_MASK_3.5717                  13320\n",
       "OR_MASK_3.6376                  13979\n",
       "OR_MASK_3.6377                  13980\n",
       "OR_MASK_3.6378                  13981\n",
       "OR_MASK_3.6867                  14470\n",
       "OR_MASK_3.6868                  14471\n",
       "OR_MASK_3.6869                  14472\n",
       "OR_MASK_3.6870                  14473\n",
       "OR_MASK_3.6871                  14474\n",
       "OR_MASK_3.6872                  14475\n",
       "OR_MASK_3.8272                  15875\n",
       "OR_MASK_3.8273                  15876\n",
       "OR_MASK_3.8274                  15877\n",
       "OR_MASK_3.8284                  15887\n",
       "z                               30809"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the best features and their scores\n",
    "k=pd.DataFrame(cols)\n",
    "k.index = Xbest.columns\n",
    "k.columns=['SCORE OF THE FEATURE']\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:4: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SCORE OF THE FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9333</th>\n",
       "      <td>9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9578</th>\n",
       "      <td>9578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9579</th>\n",
       "      <td>9579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9580</th>\n",
       "      <td>9580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>9595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9596</th>\n",
       "      <td>9596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9597</th>\n",
       "      <td>9597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td>10071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>10072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12995</th>\n",
       "      <td>12995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13316</th>\n",
       "      <td>13316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>13317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13318</th>\n",
       "      <td>13318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13319</th>\n",
       "      <td>13319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13320</th>\n",
       "      <td>13320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13979</th>\n",
       "      <td>13979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13980</th>\n",
       "      <td>13980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13981</th>\n",
       "      <td>13981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14470</th>\n",
       "      <td>14470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14471</th>\n",
       "      <td>14471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472</th>\n",
       "      <td>14472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14473</th>\n",
       "      <td>14473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14474</th>\n",
       "      <td>14474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14475</th>\n",
       "      <td>14475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15875</th>\n",
       "      <td>15875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15876</th>\n",
       "      <td>15876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15877</th>\n",
       "      <td>15877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15886</th>\n",
       "      <td>15886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15887</th>\n",
       "      <td>15887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30809</th>\n",
       "      <td>30809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SCORE OF THE FEATURE\n",
       "9333                   9333\n",
       "9578                   9578\n",
       "9579                   9579\n",
       "9580                   9580\n",
       "9595                   9595\n",
       "9596                   9596\n",
       "9597                   9597\n",
       "10071                 10071\n",
       "10072                 10072\n",
       "12995                 12995\n",
       "13316                 13316\n",
       "13317                 13317\n",
       "13318                 13318\n",
       "13319                 13319\n",
       "13320                 13320\n",
       "13979                 13979\n",
       "13980                 13980\n",
       "13981                 13981\n",
       "14470                 14470\n",
       "14471                 14471\n",
       "14472                 14472\n",
       "14473                 14473\n",
       "14474                 14474\n",
       "14475                 14475\n",
       "15875                 15875\n",
       "15876                 15876\n",
       "15877                 15877\n",
       "15886                 15886\n",
       "15887                 15887\n",
       "30809                 30809"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa,Xb,ya,yb=train_test_split(X, y, test_size=0.33)\n",
    "select_feature = SelectKBest(chi2, k=30).fit(Xa, ya)\n",
    "Xa=pd.DataFrame(Xa)\n",
    "Xa.coulumns = df_fe.drop('class', axis=1).columns\n",
    "cols = select_feature.get_support(indices=True)\n",
    "Xbe = Xa.iloc[:,cols]\n",
    "\n",
    "#Print the best features and their scores\n",
    "k=pd.DataFrame(cols)\n",
    "k.index = Xbe.columns\n",
    "k.columns=['SCORE OF THE FEATURE']\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that we have the same columns to keep if we take the Xtrain or all the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We normalize the dataset between 0 and 1\n",
    "Xbestn = scaler.fit_transform(Xbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.05248349],\n",
       "       [0.        , 0.        , 0.        , ..., 0.00349786, 0.        ,\n",
       "        0.02080698],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.08172428],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00064853],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00060441],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00049813]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See the result\n",
    "Xbestn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Try different model for the classification with all methods used for the reduction of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will split in train and test (33% of the dataset for the test) the 3 dataset which are the result of 3 methods to reduce the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separetion for SelectKbest\n",
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(Xbestn, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separation for PCA \n",
    "X_trainpca, X_testpca, y_trainpca, y_testpca = train_test_split(Xpca, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separation for LDA is done when we reduce the dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "XGBoost stands for eXtreme Gradient Boosting.Gradiant boosted tree algorithms.It's  designed to be highly efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBEST\n",
      "Scikit-Learn's Xgboost Classifier's prediction accuracy is: 91.08\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "#Xgboost for KBEST\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=0)\n",
    "xgb_model.fit(X_trainb, y_trainb)\n",
    "\n",
    "\n",
    "preds = xgb_model.predict(X_testb)\n",
    "\n",
    "acc_xgbB = (preds == y_testb).sum().astype(float) / len(preds)*100\n",
    "print(\"KBEST\")\n",
    "print(\"Scikit-Learn's Xgboost Classifier's prediction accuracy is: %3.2f\" % (acc_xgbB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Scikit-Learn's XGboost Classifier's prediction accuracy is: 79.21\n"
     ]
    }
   ],
   "source": [
    "#Xgboost for pca\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=0)\n",
    "xgb_model.fit(X_trainpca, y_trainpca)\n",
    "\n",
    "\n",
    "preds = xgb_model.predict(X_testpca)\n",
    "\n",
    "acc_xgbp = (preds == y_testpca).sum().astype(float) / len(preds)*100\n",
    "print(\"PCA\")\n",
    "print(\"Scikit-Learn's XGboost Classifier's prediction accuracy is: %3.2f\" % (acc_xgbp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA\n",
      "Scikit-Learn's XGboost Classifier's prediction accuracy is: 76.91\n"
     ]
    }
   ],
   "source": [
    "#Xgboost for LDA\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "preds = xgb_model.predict(X_test)\n",
    "\n",
    "acc_xgbe = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"LDA\")\n",
    "print(\"Scikit-Learn's XGboost Classifier's prediction accuracy is: %3.2f\" % (acc_xgbe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBEST\n",
      "Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: 88.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#KNN with KBEST\n",
    "knn = KNeighborsClassifier( leaf_size= 1,\n",
    " n_neighbors= 7)\n",
    "\n",
    "knn.fit(X_trainb, y_trainb)\n",
    "\n",
    "\n",
    "preds = knn.predict(X_testb)\n",
    "\n",
    "acc_knnB = (preds == y_testb).sum().astype(float) / len(preds)*100\n",
    "print(\"KBEST\")\n",
    "print(\"Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f\" % (acc_knnB))\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "#leaf_size = list(range(1,20))\n",
    "#n_neighbors = list(range(1,20))\n",
    "#Convert to dictionary\n",
    "#hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors)#Create new KNN object\n",
    "#knn_2 = KNeighborsClassifier()#Use GridSearch\n",
    "#clf = GridSearchCV(knn_2, hyperparameters, cv=10)#Fit the model\n",
    "#best_model = clf.fit(X_trainb, y_trainb)#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBEST\n",
      "Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: 88.64\n"
     ]
    }
   ],
   "source": [
    "#Knn with the best parametre \n",
    "knn = KNeighborsClassifier( leaf_size= 1,\n",
    " n_neighbors= 7)\n",
    "\n",
    "knn.fit(X_trainb, y_trainb)\n",
    "\n",
    "\n",
    "preds = knn.predict(X_testb)\n",
    "\n",
    "acc_knnB = (preds == y_testb).sum().astype(float) / len(preds)*100\n",
    "print(\"KBEST\")\n",
    "print(\"Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f\" % (acc_knnB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: 76.66\n"
     ]
    }
   ],
   "source": [
    "#KNN with PCA\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_trainpca, y_trainpca)\n",
    "\n",
    "\n",
    "preds = knn.predict(X_testpca)\n",
    "\n",
    "acc_knnPCA = (preds == y_testpca).sum().astype(float) / len(preds)*100\n",
    "print(\"PCA\")\n",
    "print(\"Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f\" % (acc_knnPCA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA\n",
      "Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: 77.41\n"
     ]
    }
   ],
   "source": [
    "#KNN with LDA\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "acc_knne = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"LDA\")\n",
    "print(\"Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f\" % (acc_knne))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "Primarily Naïve Bayes is a linear classifier, which is a supervised machine learning method and works as a probabilistic classifier as well. When handling real-time data with continuous distribution, Naïve Bayes classifier considers that the big data is generated through a Gaussian process with normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kbest\n",
      "Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: 36.89\n"
     ]
    }
   ],
   "source": [
    "#GNB for Kbest\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "#first we transform the data to be normally distributed to have better result\n",
    "scaler_gnb = MaxAbsScaler()\n",
    "sds = scaler_gnb.fit_transform(Xbestn)\n",
    "X_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = train_test_split(sds, y, test_size=0.33)\n",
    "\n",
    "#we create the classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_gnb, y_train_gnb)\n",
    "\n",
    "\n",
    "preds = gnb.predict(X_test_gnb)\n",
    "\n",
    "acc_gnb = (preds == y_test_gnb).sum().astype(float) / len(preds)*100\n",
    "\n",
    "print(\"Kbest\")\n",
    "print(\"Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: %3.2f\" % (acc_gnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: 52.52\n"
     ]
    }
   ],
   "source": [
    "#GNB for PCA\n",
    "scaler_gnb = MaxAbsScaler()\n",
    "sds = scaler_gnb.fit_transform(Xpca)\n",
    "X_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = train_test_split(sds, y, test_size=0.33)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_gnb, y_train_gnb)\n",
    "\n",
    "\n",
    "preds = gnb.predict(X_test_gnb)\n",
    "\n",
    "acc_gnbPCA = (preds == y_test_gnb).sum().astype(float) / len(preds)*100\n",
    "\n",
    "print(\"PCA\")\n",
    "print(\"Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: %3.2f\" % (acc_gnbPCA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA\n",
      "Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: 51.97\n"
     ]
    }
   ],
   "source": [
    "#GNB for LDA\n",
    "scaler_gnb = MaxAbsScaler()\n",
    "X_train_gnb = scaler_gnb.fit_transform(X_train)\n",
    "X_test_gnb = scaler_gnb.fit_transform(X_test)\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_gnb, y_train)\n",
    "\n",
    "\n",
    "preds = gnb.predict(X_test_gnb)\n",
    "\n",
    "acc_gnbe = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "\n",
    "print(\"LDA\")\n",
    "print(\"Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: %3.2f\" % (acc_gnbe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForrest\n",
    "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kbest\n",
      "Scikit-Learn's Random Forest Classifier's prediction accuracy is: 88.97\n"
     ]
    }
   ],
   "source": [
    "#rfc for Kbest with 15 tree\n",
    "rfc = RandomForestClassifier(n_estimators=15)\n",
    "\n",
    "rfc.fit(X_trainb, y_trainb)\n",
    "\n",
    "\n",
    "preds = rfc.predict(X_testb)\n",
    "\n",
    "acc_rfcb = (preds == y_testb).sum().astype(float) / len(preds)*100\n",
    "print(\"Kbest\")\n",
    "print(\"Scikit-Learn's Random Forest Classifier's prediction accuracy is: %3.2f\" % (acc_rfcb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA\n",
      "Scikit-Learn's Random Forest Classifier's prediction accuracy is: 76.65\n"
     ]
    }
   ],
   "source": [
    "#rfc for LDA\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "preds = rfc.predict(X_test)\n",
    "\n",
    "acc_rfce = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"LDA\")\n",
    "print(\"Scikit-Learn's Random Forest Classifier's prediction accuracy is: %3.2f\" % (acc_rfce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Scikit-Learn's Random Forest Classifier's prediction accuracy is: 74.64\n"
     ]
    }
   ],
   "source": [
    "#rfc for PCA\n",
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "rfc.fit(X_trainpca, y_trainpca)\n",
    "\n",
    "\n",
    "preds = rfc.predict(X_testpca)\n",
    "\n",
    "acc_rfcpca = (preds == y_testpca).sum().astype(float) / len(preds)*100\n",
    "print(\"PCA\")\n",
    "print(\"Scikit-Learn's Random Forest Classifier's prediction accuracy is: %3.2f\" % (acc_rfcpca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine\n",
    "SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBEST\n",
      "Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: 73.53\n"
     ]
    }
   ],
   "source": [
    "#svm for kbest\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_trainb, y_trainb)\n",
    "\n",
    "preds = svc.predict(X_testb)\n",
    "\n",
    "acc_svc = (preds == y_testb).sum().astype(float) / len(preds)*100\n",
    "print(\"KBEST\")\n",
    "print(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA\n",
      "Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: 40.25\n"
     ]
    }
   ],
   "source": [
    "#svm for PCA\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_trainpca, y_trainpca)\n",
    "\n",
    "preds = svc.predict(X_testpca)\n",
    "\n",
    "acc_svcpca = (preds == y_testb).sum().astype(float) / len(preds)*100\n",
    "print(\"PCA\")\n",
    "print(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svcpca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA\n",
      "Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: 68.65\n"
     ]
    }
   ],
   "source": [
    "#svm for LDA\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "preds = svc.predict(X_test)\n",
    "\n",
    "acc_svce = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"LDA\")\n",
    "print(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "CatBoost is an algorithm for gradient boosting on decision trees. Developed by Yandex researchers and engineers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost model is fitted: True\n",
      "CatBoost model parameters:\n",
      "{'learning_rate': 0.05, 'iterations': 1000}\n",
      "kbest\n",
      "Scikit-Learn's catboost Classifier's prediction accuracy is: 91.19\n"
     ]
    }
   ],
   "source": [
    "#catboost with kbest\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "clf = CatBoostClassifier(iterations=1000, \n",
    "    learning_rate=0.05)\n",
    "\n",
    "\n",
    "clf.fit(X_trainb, y_trainb,  \n",
    "        eval_set=(X_testb, y_testb), \n",
    "        verbose=False\n",
    ")\n",
    "\n",
    "print('CatBoost model is fitted: ' + str(clf.is_fitted()))\n",
    "print('CatBoost model parameters:')\n",
    "print(clf.get_params())\n",
    "preds = clf.predict(data=X_testb)\n",
    "preds = preds[:,0]\n",
    "acc_clfb = (preds == y_testb).sum().astype(float) / len(preds)*100\n",
    "print(\"kbest\")\n",
    "print(\"Scikit-Learn's catboost Classifier's prediction accuracy is: %3.2f\" % (acc_clfb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost model is fitted: True\n",
      "CatBoost model parameters:\n",
      "{'learning_rate': 0.05, 'iterations': 1000}\n",
      "PCA\n",
      "Scikit-Learn's catboost Classifier's prediction accuracy is: 79.55\n"
     ]
    }
   ],
   "source": [
    "#catboost with PCA\n",
    "clf = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    learning_rate=0.05)\n",
    "\n",
    "\n",
    "clf.fit(X_trainpca, y_trainpca,  \n",
    "        eval_set=(X_testpca, y_testpca), \n",
    "        verbose=False\n",
    ")\n",
    "\n",
    "print('CatBoost model is fitted: ' + str(clf.is_fitted()))\n",
    "print('CatBoost model parameters:')\n",
    "print(clf.get_params())\n",
    "preds = clf.predict(data=X_testpca)\n",
    "preds = preds[:,0]\n",
    "acc_clfp = (preds == y_testpca).sum().astype(float) / len(preds)*100\n",
    "print(\"PCA\")\n",
    "print(\"Scikit-Learn's catboost Classifier's prediction accuracy is: %3.2f\" % (acc_clfp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost model is fitted: True\n",
      "CatBoost model parameters:\n",
      "{'learning_rate': 0.05, 'iterations': 1000}\n",
      "LDA\n",
      "Scikit-Learn's catboost Classifier's prediction accuracy is: 73.31\n"
     ]
    }
   ],
   "source": [
    "#catboost with LDA\n",
    "clf = CatBoostClassifier(\n",
    "    iterations=1000, \n",
    "    learning_rate=0.05)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train,  \n",
    "        eval_set=(X_test, y_test), \n",
    "        verbose=False\n",
    ")\n",
    "\n",
    "print('CatBoost model is fitted: ' + str(clf.is_fitted()))\n",
    "print('CatBoost model parameters:')\n",
    "print(clf.get_params())\n",
    "preds = clf.predict(data=X_test)\n",
    "preds = preds[:,0]\n",
    "acc_clfe = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "print(\"LDA\")\n",
    "print(\"Scikit-Learn's catboost Classifier's prediction accuracy is: %3.2f\" % (acc_clfe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "We implement thanks to tensorflow a multiple layers perceptron for the classification.After tried differents parameter we choosed a NN of 5 layers:\n",
    "- The number of neurones of the first layer is the numbers of column of the dataset\n",
    "- The The number of neurones of the second and third  layers are 10 and for the fourth the number is 6\n",
    "- to conclude the number of neurones for the ultimate layer is 3 because their are 3 class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#Kbest data preparation\n",
    "#Data preparation for the neural network\n",
    "#encode the target class\n",
    "le = LabelEncoder()\n",
    "le.fit(y_trainb)\n",
    "y_trainb = le.transform(y_trainb)\n",
    "le.fit(y_testb)\n",
    "y_testb = le.transform(y_testb)\n",
    "\n",
    "y_trainb = to_categorical(y_trainb)\n",
    "y_testb = to_categorical(y_testb)\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_trainb) # fit scaler to training data only\n",
    "X_trainb = pd.DataFrame(scaler.transform(X_trainb))\n",
    "X_testb = pd.DataFrame(scaler.transform(X_testb))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA data preparation\n",
    "#Data preparation for the neural network\n",
    "#encode the target class\n",
    "le = LabelEncoder()\n",
    "le.fit(y_trainpca)\n",
    "y_trainpca = le.transform(y_trainpca)\n",
    "le.fit(y_testpca)\n",
    "y_testpca = le.transform(y_testpca)\n",
    "\n",
    "y_trainpca = to_categorical(y_trainpca)\n",
    "y_testpca = to_categorical(y_testpca)\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_trainpca) # fit scaler to training data only\n",
    "X_trainpca = pd.DataFrame(scaler.transform(X_trainpca))\n",
    "X_testpca = pd.DataFrame(scaler.transform(X_testpca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA data preparation\n",
    "#Data preparation for the neural network\n",
    "#encode the target class\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "le.fit(y_test)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # fit scaler to training data only\n",
    "X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "X_test = pd.DataFrame(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4904 - categorical_accuracy: 0.8475 - val_loss: 0.9517 - val_categorical_accuracy: 0.7643\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0427 - categorical_accuracy: 0.9951 - val_loss: 1.5210 - val_categorical_accuracy: 0.7581\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0135 - categorical_accuracy: 0.9967 - val_loss: 1.8624 - val_categorical_accuracy: 0.7592\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0076 - categorical_accuracy: 0.9977 - val_loss: 2.1110 - val_categorical_accuracy: 0.7590\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0065 - categorical_accuracy: 0.9985 - val_loss: 2.3283 - val_categorical_accuracy: 0.7624\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0052 - categorical_accuracy: 0.9986 - val_loss: 2.5036 - val_categorical_accuracy: 0.7608\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0047 - categorical_accuracy: 0.9987 - val_loss: 2.6860 - val_categorical_accuracy: 0.7644\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0033 - categorical_accuracy: 0.9992 - val_loss: 2.8556 - val_categorical_accuracy: 0.7665\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0042 - categorical_accuracy: 0.9990 - val_loss: 2.9488 - val_categorical_accuracy: 0.7653\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0033 - categorical_accuracy: 0.9994 - val_loss: 3.0823 - val_categorical_accuracy: 0.7653\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0032 - categorical_accuracy: 0.9993 - val_loss: 3.1908 - val_categorical_accuracy: 0.7642\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0024 - categorical_accuracy: 0.9997 - val_loss: 3.3024 - val_categorical_accuracy: 0.7651\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0025 - categorical_accuracy: 0.9995 - val_loss: 3.4211 - val_categorical_accuracy: 0.7661\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0021 - categorical_accuracy: 0.9996 - val_loss: 3.5564 - val_categorical_accuracy: 0.7688\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0022 - categorical_accuracy: 0.9996 - val_loss: 3.7705 - val_categorical_accuracy: 0.7697\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0025 - categorical_accuracy: 0.9996 - val_loss: 3.8070 - val_categorical_accuracy: 0.7674\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0024 - categorical_accuracy: 0.9993 - val_loss: 3.8760 - val_categorical_accuracy: 0.7672\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.9995 - val_loss: 3.9567 - val_categorical_accuracy: 0.7665\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0017 - categorical_accuracy: 0.9995 - val_loss: 4.0812 - val_categorical_accuracy: 0.7678\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0016 - categorical_accuracy: 0.9995 - val_loss: 4.1355 - val_categorical_accuracy: 0.7605\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0023 - categorical_accuracy: 0.9995 - val_loss: 4.2174 - val_categorical_accuracy: 0.7678\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.9996 - val_loss: 4.1437 - val_categorical_accuracy: 0.7678\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0012 - categorical_accuracy: 0.9997 - val_loss: 4.2095 - val_categorical_accuracy: 0.7677\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0016 - categorical_accuracy: 0.9995 - val_loss: 4.2402 - val_categorical_accuracy: 0.7678\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0020 - categorical_accuracy: 0.9995 - val_loss: 4.2969 - val_categorical_accuracy: 0.7677\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 8.4680e-04 - categorical_accuracy: 0.9999 - val_loss: 4.4740 - val_categorical_accuracy: 0.7680\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0015 - categorical_accuracy: 0.9996 - val_loss: 4.4851 - val_categorical_accuracy: 0.7677\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0013 - categorical_accuracy: 0.9997 - val_loss: 4.5537 - val_categorical_accuracy: 0.7659\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0013 - categorical_accuracy: 0.9997 - val_loss: 4.6199 - val_categorical_accuracy: 0.7667\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0012 - categorical_accuracy: 0.9997 - val_loss: 4.6683 - val_categorical_accuracy: 0.7639\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0012 - categorical_accuracy: 0.9997 - val_loss: 4.8340 - val_categorical_accuracy: 0.7666\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0014 - categorical_accuracy: 0.9996 - val_loss: 4.6936 - val_categorical_accuracy: 0.7642\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 9.8057e-04 - categorical_accuracy: 0.9997 - val_loss: 4.6996 - val_categorical_accuracy: 0.7648\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0014 - categorical_accuracy: 0.9995 - val_loss: 4.7719 - val_categorical_accuracy: 0.7648\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0021 - categorical_accuracy: 0.9995 - val_loss: 4.7706 - val_categorical_accuracy: 0.7676\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0015 - categorical_accuracy: 0.9997 - val_loss: 4.8864 - val_categorical_accuracy: 0.7669\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 7.7006e-04 - categorical_accuracy: 0.9997 - val_loss: 4.8775 - val_categorical_accuracy: 0.7665\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0011 - categorical_accuracy: 0.9997 - val_loss: 4.9772 - val_categorical_accuracy: 0.7666\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0019 - categorical_accuracy: 0.9993 - val_loss: 4.8971 - val_categorical_accuracy: 0.7670\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 7.5898e-04 - categorical_accuracy: 0.9997 - val_loss: 5.0433 - val_categorical_accuracy: 0.7666\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 9.7510e-04 - categorical_accuracy: 0.9997 - val_loss: 5.0981 - val_categorical_accuracy: 0.7666\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0017 - categorical_accuracy: 0.9995 - val_loss: 4.7924 - val_categorical_accuracy: 0.7627\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0012 - categorical_accuracy: 0.9996 - val_loss: 4.7647 - val_categorical_accuracy: 0.7619\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0013 - categorical_accuracy: 0.9996 - val_loss: 5.0784 - val_categorical_accuracy: 0.7685\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 5.9447e-04 - categorical_accuracy: 0.9998 - val_loss: 5.2014 - val_categorical_accuracy: 0.7680\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 8.5404e-04 - categorical_accuracy: 0.9998 - val_loss: 5.2878 - val_categorical_accuracy: 0.7684\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 8.7664e-04 - categorical_accuracy: 0.9998 - val_loss: 5.1765 - val_categorical_accuracy: 0.7667\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0015 - categorical_accuracy: 0.9996 - val_loss: 5.2539 - val_categorical_accuracy: 0.7673\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 8.8111e-04 - categorical_accuracy: 0.9997 - val_loss: 5.3153 - val_categorical_accuracy: 0.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0013 - categorical_accuracy: 0.9998 - val_loss: 5.2709 - val_categorical_accuracy: 0.7654\n",
      "LDA\n",
      "Deep Neural Network\n",
      "Validation Accuracy: 76.53919\n"
     ]
    }
   ],
   "source": [
    "# create a deep neural network model\n",
    "num_features = X_train.shape[1]\n",
    "dnn = tf.keras.Sequential()\n",
    "dnn.add(Dense(10, input_dim=num_features, activation='relu'))\n",
    "dnn.add(Dropout(0.1))#the dropout to avoid overfitting\n",
    "dnn.add(Dense(10, activation='relu'))\n",
    "dnn.add(Dropout(0.1))\n",
    "dnn.add(Dense(6, activation='relu'))\n",
    "dnn.add(Dense(3, activation='softmax', name='output'))#3 because their are 3 class to precict\n",
    "\n",
    "dnn.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "# train DNN\n",
    "my_epochs = 50\n",
    "history = dnn.fit(X_train, y_train, epochs=my_epochs, batch_size=50,validation_data=(X_test, y_test))\n",
    "preds = pd.DataFrame(dnn.predict(X_test))\n",
    "preds = preds.idxmax(axis=1)\n",
    "y_testt = y_test.dot([0,1,2])\n",
    "model_acce = (preds == y_testt).sum().astype(float) / len(preds) * 100\n",
    "print('LDA')\n",
    "print('Deep Neural Network')\n",
    "print('Validation Accuracy: %3.5f' % (model_acce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.9401 - categorical_accuracy: 0.6167 - val_loss: 0.7447 - val_categorical_accuracy: 0.7236\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6561 - categorical_accuracy: 0.7165 - val_loss: 0.4988 - val_categorical_accuracy: 0.7475\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4822 - categorical_accuracy: 0.8082 - val_loss: 0.3599 - val_categorical_accuracy: 0.8832\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.4010 - categorical_accuracy: 0.8440 - val_loss: 0.3142 - val_categorical_accuracy: 0.8900\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.3612 - categorical_accuracy: 0.8615 - val_loss: 0.2929 - val_categorical_accuracy: 0.8950\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.3416 - categorical_accuracy: 0.8709 - val_loss: 0.2842 - val_categorical_accuracy: 0.8998\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.3286 - categorical_accuracy: 0.8746 - val_loss: 0.2783 - val_categorical_accuracy: 0.8963\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.3122 - categorical_accuracy: 0.8836 - val_loss: 0.2730 - val_categorical_accuracy: 0.8988\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.3076 - categorical_accuracy: 0.8860 - val_loss: 0.2691 - val_categorical_accuracy: 0.9009\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2965 - categorical_accuracy: 0.8907 - val_loss: 0.2673 - val_categorical_accuracy: 0.8990\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2914 - categorical_accuracy: 0.8920 - val_loss: 0.2653 - val_categorical_accuracy: 0.9040\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2878 - categorical_accuracy: 0.8925 - val_loss: 0.2657 - val_categorical_accuracy: 0.9015\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2830 - categorical_accuracy: 0.8946 - val_loss: 0.2602 - val_categorical_accuracy: 0.9030\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2801 - categorical_accuracy: 0.8951 - val_loss: 0.2609 - val_categorical_accuracy: 0.9030\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2788 - categorical_accuracy: 0.8958 - val_loss: 0.2603 - val_categorical_accuracy: 0.9032\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2731 - categorical_accuracy: 0.8987 - val_loss: 0.2649 - val_categorical_accuracy: 0.9010\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2741 - categorical_accuracy: 0.8979 - val_loss: 0.2614 - val_categorical_accuracy: 0.9052\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2691 - categorical_accuracy: 0.8988 - val_loss: 0.2559 - val_categorical_accuracy: 0.9051\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2655 - categorical_accuracy: 0.9009 - val_loss: 0.2573 - val_categorical_accuracy: 0.9053\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2663 - categorical_accuracy: 0.9008 - val_loss: 0.2571 - val_categorical_accuracy: 0.9041\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2631 - categorical_accuracy: 0.8994 - val_loss: 0.2550 - val_categorical_accuracy: 0.9028\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2634 - categorical_accuracy: 0.8999 - val_loss: 0.2590 - val_categorical_accuracy: 0.9024\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2615 - categorical_accuracy: 0.8999 - val_loss: 0.2568 - val_categorical_accuracy: 0.9044\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2611 - categorical_accuracy: 0.9019 - val_loss: 0.2563 - val_categorical_accuracy: 0.9039\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2613 - categorical_accuracy: 0.9005 - val_loss: 0.2545 - val_categorical_accuracy: 0.9052\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2567 - categorical_accuracy: 0.9009 - val_loss: 0.2585 - val_categorical_accuracy: 0.9059\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2577 - categorical_accuracy: 0.9033 - val_loss: 0.2544 - val_categorical_accuracy: 0.9034\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2538 - categorical_accuracy: 0.9028 - val_loss: 0.2594 - val_categorical_accuracy: 0.9030\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2558 - categorical_accuracy: 0.9019 - val_loss: 0.2517 - val_categorical_accuracy: 0.9082\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2563 - categorical_accuracy: 0.9032 - val_loss: 0.2533 - val_categorical_accuracy: 0.9066\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2564 - categorical_accuracy: 0.9015 - val_loss: 0.2537 - val_categorical_accuracy: 0.9074\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2557 - categorical_accuracy: 0.9017 - val_loss: 0.2539 - val_categorical_accuracy: 0.9056\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2544 - categorical_accuracy: 0.9030 - val_loss: 0.2516 - val_categorical_accuracy: 0.9074\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2531 - categorical_accuracy: 0.9033 - val_loss: 0.2525 - val_categorical_accuracy: 0.9037\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2525 - categorical_accuracy: 0.9039 - val_loss: 0.2558 - val_categorical_accuracy: 0.9034\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2559 - categorical_accuracy: 0.9027 - val_loss: 0.2535 - val_categorical_accuracy: 0.9048\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2529 - categorical_accuracy: 0.9034 - val_loss: 0.2517 - val_categorical_accuracy: 0.9068\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2515 - categorical_accuracy: 0.9040 - val_loss: 0.2525 - val_categorical_accuracy: 0.9051\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2521 - categorical_accuracy: 0.9027 - val_loss: 0.2539 - val_categorical_accuracy: 0.9072\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2527 - categorical_accuracy: 0.9027 - val_loss: 0.2518 - val_categorical_accuracy: 0.9074\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2527 - categorical_accuracy: 0.9030 - val_loss: 0.2507 - val_categorical_accuracy: 0.9060\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2542 - categorical_accuracy: 0.9020 - val_loss: 0.2513 - val_categorical_accuracy: 0.9052\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2531 - categorical_accuracy: 0.9012 - val_loss: 0.2496 - val_categorical_accuracy: 0.9086\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2502 - categorical_accuracy: 0.9047 - val_loss: 0.2533 - val_categorical_accuracy: 0.9039\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2485 - categorical_accuracy: 0.9047 - val_loss: 0.2513 - val_categorical_accuracy: 0.9087\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2484 - categorical_accuracy: 0.9037 - val_loss: 0.2525 - val_categorical_accuracy: 0.9055\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2514 - categorical_accuracy: 0.9023 - val_loss: 0.2507 - val_categorical_accuracy: 0.9049\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2475 - categorical_accuracy: 0.9051 - val_loss: 0.2536 - val_categorical_accuracy: 0.9049\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2481 - categorical_accuracy: 0.9036 - val_loss: 0.2484 - val_categorical_accuracy: 0.9079\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2517 - categorical_accuracy: 0.9034 - val_loss: 0.2513 - val_categorical_accuracy: 0.9066\n",
      "kbest\n",
      "Deep Neural Network\n",
      "Validation Accuracy: 90.65636\n"
     ]
    }
   ],
   "source": [
    "# create a deep neural network model\n",
    "num_features = X_trainb.shape[1]\n",
    "dnn = tf.keras.Sequential()\n",
    "dnn.add(Dense(10, input_dim=num_features, activation='relu'))\n",
    "dnn.add(Dropout(0.1))#the dropout to avoid overfitting\n",
    "dnn.add(Dense(10, activation='relu'))\n",
    "dnn.add(Dropout(0.1))\n",
    "dnn.add(Dense(6, activation='relu'))\n",
    "dnn.add(Dense(3, activation='softmax', name='output'))#3 because their are 3 class to precict\n",
    "\n",
    "dnn.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "# train DNN\n",
    "my_epochs = 50\n",
    "history = dnn.fit(X_trainb, y_trainb, epochs=my_epochs, batch_size=50,validation_data=(X_testb, y_testb))\n",
    "preds = pd.DataFrame(dnn.predict(X_testb))\n",
    "preds = preds.idxmax(axis=1)\n",
    "y_testbt = y_testb.dot([0,1,2])\n",
    "model_acc = (preds == y_testbt).sum().astype(float) / len(preds) * 100\n",
    "print('kbest')\n",
    "print('Deep Neural Network')\n",
    "print('Validation Accuracy: %3.5f' % (model_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 1.0579 - categorical_accuracy: 0.4726 - val_loss: 1.0073 - val_categorical_accuracy: 0.5190\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.9883 - categorical_accuracy: 0.5292 - val_loss: 0.9558 - val_categorical_accuracy: 0.5567\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.9438 - categorical_accuracy: 0.5608 - val_loss: 0.9078 - val_categorical_accuracy: 0.5881\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.9012 - categorical_accuracy: 0.5932 - val_loss: 0.8623 - val_categorical_accuracy: 0.6256\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.8675 - categorical_accuracy: 0.6125 - val_loss: 0.8280 - val_categorical_accuracy: 0.6391\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.8342 - categorical_accuracy: 0.6322 - val_loss: 0.8070 - val_categorical_accuracy: 0.6663\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.8151 - categorical_accuracy: 0.6501 - val_loss: 0.7857 - val_categorical_accuracy: 0.6805\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.8042 - categorical_accuracy: 0.6551 - val_loss: 0.7722 - val_categorical_accuracy: 0.6908\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7873 - categorical_accuracy: 0.6676 - val_loss: 0.7541 - val_categorical_accuracy: 0.6984\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7800 - categorical_accuracy: 0.6702 - val_loss: 0.7382 - val_categorical_accuracy: 0.7057\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7641 - categorical_accuracy: 0.6829 - val_loss: 0.7245 - val_categorical_accuracy: 0.7183\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7490 - categorical_accuracy: 0.6861 - val_loss: 0.7102 - val_categorical_accuracy: 0.7231\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7367 - categorical_accuracy: 0.6972 - val_loss: 0.6961 - val_categorical_accuracy: 0.7263\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7289 - categorical_accuracy: 0.6962 - val_loss: 0.6869 - val_categorical_accuracy: 0.7327\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7199 - categorical_accuracy: 0.6990 - val_loss: 0.6780 - val_categorical_accuracy: 0.7335\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7143 - categorical_accuracy: 0.7040 - val_loss: 0.6754 - val_categorical_accuracy: 0.7364\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.7117 - categorical_accuracy: 0.7034 - val_loss: 0.6728 - val_categorical_accuracy: 0.7352\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.7060 - categorical_accuracy: 0.7048 - val_loss: 0.6673 - val_categorical_accuracy: 0.7365\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6993 - categorical_accuracy: 0.7101 - val_loss: 0.6649 - val_categorical_accuracy: 0.7394\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6986 - categorical_accuracy: 0.7074 - val_loss: 0.6633 - val_categorical_accuracy: 0.7398\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6943 - categorical_accuracy: 0.7131 - val_loss: 0.6556 - val_categorical_accuracy: 0.7415\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6875 - categorical_accuracy: 0.7124 - val_loss: 0.6531 - val_categorical_accuracy: 0.7494\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6848 - categorical_accuracy: 0.7121 - val_loss: 0.6510 - val_categorical_accuracy: 0.7487\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6841 - categorical_accuracy: 0.7187 - val_loss: 0.6503 - val_categorical_accuracy: 0.7471\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6835 - categorical_accuracy: 0.7166 - val_loss: 0.6487 - val_categorical_accuracy: 0.7505\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6774 - categorical_accuracy: 0.7217 - val_loss: 0.6442 - val_categorical_accuracy: 0.7535\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6706 - categorical_accuracy: 0.7232 - val_loss: 0.6398 - val_categorical_accuracy: 0.7551\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6682 - categorical_accuracy: 0.7263 - val_loss: 0.6380 - val_categorical_accuracy: 0.7531\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6668 - categorical_accuracy: 0.7284 - val_loss: 0.6338 - val_categorical_accuracy: 0.7539\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6597 - categorical_accuracy: 0.7309 - val_loss: 0.6329 - val_categorical_accuracy: 0.7562\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6615 - categorical_accuracy: 0.7283 - val_loss: 0.6288 - val_categorical_accuracy: 0.7571\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6609 - categorical_accuracy: 0.7312 - val_loss: 0.6264 - val_categorical_accuracy: 0.7537\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6520 - categorical_accuracy: 0.7376 - val_loss: 0.6262 - val_categorical_accuracy: 0.7558\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6500 - categorical_accuracy: 0.7385 - val_loss: 0.6235 - val_categorical_accuracy: 0.7570\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6505 - categorical_accuracy: 0.7348 - val_loss: 0.6188 - val_categorical_accuracy: 0.7586\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6391 - categorical_accuracy: 0.7406 - val_loss: 0.6150 - val_categorical_accuracy: 0.7624\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6397 - categorical_accuracy: 0.7409 - val_loss: 0.6128 - val_categorical_accuracy: 0.7632\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6399 - categorical_accuracy: 0.7418 - val_loss: 0.6135 - val_categorical_accuracy: 0.7640\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6369 - categorical_accuracy: 0.7419 - val_loss: 0.6101 - val_categorical_accuracy: 0.7605\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6369 - categorical_accuracy: 0.7422 - val_loss: 0.6108 - val_categorical_accuracy: 0.7651\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6335 - categorical_accuracy: 0.7464 - val_loss: 0.6076 - val_categorical_accuracy: 0.7640\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6319 - categorical_accuracy: 0.7468 - val_loss: 0.6052 - val_categorical_accuracy: 0.7685\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6340 - categorical_accuracy: 0.7452 - val_loss: 0.6055 - val_categorical_accuracy: 0.7681\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6303 - categorical_accuracy: 0.7486 - val_loss: 0.6030 - val_categorical_accuracy: 0.7718\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6278 - categorical_accuracy: 0.7517 - val_loss: 0.6014 - val_categorical_accuracy: 0.7707\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6303 - categorical_accuracy: 0.7486 - val_loss: 0.6016 - val_categorical_accuracy: 0.7718\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.6247 - categorical_accuracy: 0.7501 - val_loss: 0.6032 - val_categorical_accuracy: 0.7707\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6294 - categorical_accuracy: 0.7496 - val_loss: 0.6009 - val_categorical_accuracy: 0.7731\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6293 - categorical_accuracy: 0.7528 - val_loss: 0.5999 - val_categorical_accuracy: 0.7693\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6216 - categorical_accuracy: 0.7567 - val_loss: 0.6017 - val_categorical_accuracy: 0.7688\n",
      "PCA\n",
      "Deep Neural Network\n",
      "Validation Accuracy: 76.87822\n"
     ]
    }
   ],
   "source": [
    "# create a deep neural network model\n",
    "num_features = X_trainpca.shape[1]\n",
    "dnn = tf.keras.Sequential()\n",
    "dnn.add(Dense(10, input_dim=num_features, activation='relu'))\n",
    "dnn.add(Dropout(0.1))#the dropout to avoid overfitting\n",
    "dnn.add(Dense(10, activation='relu'))\n",
    "dnn.add(Dropout(0.1))\n",
    "dnn.add(Dense(6, activation='relu'))\n",
    "dnn.add(Dense(3, activation='softmax', name='output'))#3 because their are 3 class to precict\n",
    "\n",
    "dnn.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "# train DNN\n",
    "my_epochs = 50\n",
    "history = dnn.fit(X_trainpca, y_trainpca, epochs=my_epochs, batch_size=50,validation_data=(X_testpca, y_testpca))\n",
    "preds = pd.DataFrame(dnn.predict(X_testpca))\n",
    "preds = preds.idxmax(axis=1)\n",
    "y_testpcat = y_testpca.dot([0,1,2])\n",
    "model_accpca = (preds == y_testpcat).sum().astype(float) / len(preds) * 100\n",
    "print('PCA')\n",
    "print('Deep Neural Network')\n",
    "print('Validation Accuracy: %3.5f' % (model_accpca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a list of all result\n",
    "SVM = [acc_svce, acc_svcpca, acc_svc]\n",
    "KNN = [acc_knne, acc_knnPCA, acc_knnB]\n",
    "RFC = [acc_rfce, acc_rfcpca, acc_rfcb]\n",
    "GNB = [acc_gnbe, acc_gnbPCA, acc_gnb ]\n",
    "CATBOOST = [acc_clfe,acc_clfp,acc_clfb]\n",
    "XGBOOST=[acc_xgbe,acc_xgbp,acc_xgbB]\n",
    "NeuralNetwork=[model_acce,model_accpca,model_acc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of a dataframe of the result\n",
    "result=[SVM,KNN,RFC,GNB,CATBOOST,XGBOOST,NeuralNetwork]\n",
    "result\n",
    "Res=pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "Res.columns=['LDA','PCA','SelectKbest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA</th>\n",
       "      <th>PCA</th>\n",
       "      <th>SelectKbest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.646596</td>\n",
       "      <td>40.249525</td>\n",
       "      <td>73.528614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77.407106</td>\n",
       "      <td>76.661242</td>\n",
       "      <td>88.635747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.647681</td>\n",
       "      <td>74.640629</td>\n",
       "      <td>88.974776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.966368</td>\n",
       "      <td>52.522376</td>\n",
       "      <td>36.886357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.311635</td>\n",
       "      <td>79.549769</td>\n",
       "      <td>91.185245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76.905343</td>\n",
       "      <td>79.210740</td>\n",
       "      <td>91.076756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>76.539192</td>\n",
       "      <td>76.878221</td>\n",
       "      <td>90.656360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LDA        PCA  SelectKbest\n",
       "0  68.646596  40.249525    73.528614\n",
       "1  77.407106  76.661242    88.635747\n",
       "2  76.647681  74.640629    88.974776\n",
       "3  51.966368  52.522376    36.886357\n",
       "4  73.311635  79.549769    91.185245\n",
       "5  76.905343  79.210740    91.076756\n",
       "6  76.539192  76.878221    90.656360"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LDA</th>\n",
       "      <th>PCA</th>\n",
       "      <th>SelectKbest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>68.646596</td>\n",
       "      <td>40.249525</td>\n",
       "      <td>73.528614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>77.407106</td>\n",
       "      <td>76.661242</td>\n",
       "      <td>88.635747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFC</th>\n",
       "      <td>76.647681</td>\n",
       "      <td>74.640629</td>\n",
       "      <td>88.974776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>51.966368</td>\n",
       "      <td>52.522376</td>\n",
       "      <td>36.886357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CATBOOST</th>\n",
       "      <td>73.311635</td>\n",
       "      <td>79.549769</td>\n",
       "      <td>91.185245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBOOST</th>\n",
       "      <td>76.905343</td>\n",
       "      <td>79.210740</td>\n",
       "      <td>91.076756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeuralNetwork</th>\n",
       "      <td>76.539192</td>\n",
       "      <td>76.878221</td>\n",
       "      <td>90.656360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     LDA        PCA  SelectKbest\n",
       "SVM            68.646596  40.249525    73.528614\n",
       "KNN            77.407106  76.661242    88.635747\n",
       "RFC            76.647681  74.640629    88.974776\n",
       "GNB            51.966368  52.522376    36.886357\n",
       "CATBOOST       73.311635  79.549769    91.185245\n",
       "XGBOOST        76.905343  79.210740    91.076756\n",
       "NeuralNetwork  76.539192  76.878221    90.656360"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename the Index\n",
    "Res.rename(index={0: 'SVM',1:'KNN',2:'RFC',3:'GNB',4:'CATBOOST',5:'XGBOOST',6:'NeuralNetwork'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On this dataframe we can see the accuracy for all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:7: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe7645f8c50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT THE RESULT\n",
    "p=['SVM', 'KNN', 'RANDOM_FOREST', 'Gaussian','Catboost','XGBOOST','NeuralNetwork']\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "plt.plot(p,Res.LDA, marker=\"o\", linestyle=\"\", label='LDA')\n",
    "plt.plot(p,Res.PCA, marker=\"o\", linestyle=\"\", label='PCA')\n",
    "plt.plot(p,Res.SelectKbest, marker=\"o\", linestyle=\"\", label='SelectKbest')\n",
    "axes = figure.add_subplot(111)\n",
    "\n",
    "axes.set_xticklabels(p, rotation=45)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparaison of different model\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see SelectKbest is the best method for the reduction of the dataset and the best classifier with this methods is the catboost with a accuracy of 0,91 which is good\n",
    "### Gaussian doesn't work very well as for LDA because the features are not normally distributed\n",
    "### Pca doesn't work very well because it's unsupervised\n",
    "### random forest and catboost and XGBOOST work very well because the two are using a lots of decision tree and most catboost and xgboost because they use gradient boosting also to be more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
